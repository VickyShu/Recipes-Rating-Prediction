<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="UTF-8">

<!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Recipes Rating Prediction | Recipes-Rating-Prediction</title>
<meta name="generator" content="Jekyll v3.9.3" />
<meta property="og:title" content="Recipes Rating Prediction" />
<meta property="og:locale" content="en_US" />
<link rel="canonical" href="http://localhost:4000/" />
<meta property="og:url" content="http://localhost:4000/" />
<meta property="og:site_name" content="Recipes-Rating-Prediction" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Recipes Rating Prediction" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebSite","headline":"Recipes Rating Prediction","name":"Recipes-Rating-Prediction","url":"http://localhost:4000/"}</script>
<!-- End Jekyll SEO tag -->

    <link rel="preconnect" href="https://fonts.gstatic.com">
    <link rel="preload" href="https://fonts.googleapis.com/css?family=Open+Sans:400,700&display=swap" as="style" type="text/css" crossorigin>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="theme-color" content="#157878">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <link rel="stylesheet" href="/assets/css/style.css?v=2c0e836b8192518d8afab18a08679a8880adb6cb">
    <!-- start custom head snippets, customize with your own _includes/head-custom.html file -->

<!-- Setup Google Analytics -->



<!-- You can set your favicon here -->
<!-- link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" -->

<!-- end custom head snippets -->

  </head>
  <body>
    <a id="skip-to-content" href="#content">Skip to the content.</a>

    <header class="page-header" role="banner">
      <h1 class="project-name">Recipes Rating Prediction</h1>
      <h2 class="project-tagline"></h2>
      
        <a href="https://github.com/VickyShu/Recipes-Rating-Prediction" class="btn">View on GitHub</a>
      
      
    </header>

    <main id="content" class="main-content" role="main">
      <h1 id="recipes-rating-prediction">Recipes Rating Prediction</h1>

<p><strong>Authors</strong>: Xin Shu, Chang Shu</p>

<h2 id="project-overview">Project Overview</h2>
<p>This project aims to predict the ratings giving by people based on several features. This is a project for DSC80 at UCSD. The dataset used to investigate this topic can be find <a href="https://drive.google.com/file/d/1kIbMz6jlhleiZ9_3QthmUnifoSds_2EI/view">here</a>.</p>

<h2 id="framing-the-problem">Framing the Problem</h2>
<p>In the culinary world, the balance between taste and health is a subject of much interest and debate. Meals are a critical factor in this balance, directly affecting an individual’s health and well-being. In this project, we aim to predict the rating of a recipe based on various features, like average rating of a recipe, number of ratings for each recipe, users’ average rating for recipes, and minutes to prepare a recipe. Therefore, we will build a regression model to fit our data to predict the rating for unseen recipes.</p>

<h3 id="response-variable"><strong>Response Variable</strong></h3>
<p>We chose <code class="language-plaintext highlighter-rouge">rating</code> as the response variable, which has five different classes from star one to star five. Understanding the rating of a recipe helps people to regulate their meals for better health.</p>

<h3 id="evaluation-metrics"><strong>Evaluation Metrics</strong></h3>
<p>To evaluate the model’s performance, we will use metrics such as <strong>RMSE</strong>. That’s because there are lots of outliers in our features, like lots of time to prepare recipes. So we choose RMSE because it is sensitive to outliers. It will give a higher error value, signaling that these points are not being predicted accurately.</p>

<h2 id="baseline-model">Baseline Model</h2>

<h3 id="model-description">Model Description</h3>
<p>In our baseline model, we intend to predict whether the <code class="language-plaintext highlighter-rouge">rating</code> is based on the average rating of the recipe and total minutes needed to prepare for a recipe. We will use <code class="language-plaintext highlighter-rouge">recipe_average_rating</code> and <code class="language-plaintext highlighter-rouge">log_minute</code> as features of our baseline Linear Regression model.</p>

<h3 id="feature-transformations">Feature Transformations</h3>
<p>For this baseline model, we did not have any categorical features, so we did not need to use OneHotEncoder() or any other categorical transformations.</p>

<p><code class="language-plaintext highlighter-rouge">recipe_average_rating</code>: This is an ordinary feature. From the scatter plot, we can see that there is some relationship between the average rating and the rating, which is that higher average ratings tend to have higher ratings. We just keep it as the raw integer value for simplicity.</p>

<iframe src="assets/fig_average_rating.html" width="800" height="600" frameborder="0"></iframe>

<p><code class="language-plaintext highlighter-rouge">minutes</code>: This is a nominal feature. First we plot the box plot below to see the relationship between minutes. We found that there are lots of recipes that have a long preparation time, which represents the outliers within each rating level. So we decided to use log transformation to transform minutes by log to make it more suitable for prediction. From the new scatter plot, we can see that the longest preparation time is around 15.</p>

<iframe src="assets/fig_minutes_rating.html" width="800" height="600" frameborder="0"></iframe>
<iframe src="assets/fig_log_min.html" width="800" height="600" frameborder="0"></iframe>

<h3 id="performance">Performance</h3>
<p>Our model achieved a training RMSE of 0.3715 and a testing RMSE of 0.5147. This level of RMSE indicates that the model has a reasonably good predictive capability. That is, if a recipe has a high average rating, it is more likely to receive similarly high ratings from new reviewers, keeping its high score. Our model’s performance corroborates this theory, following the tendency of recipes with higher ratings to maintain higher future ratings.</p>

<table>
  <thead>
    <tr>
      <th>Metric</th>
      <th>Train RMSE</th>
      <th>Test RMSE</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>‘RMSE’</td>
      <td>0.371539447765363</td>
      <td>0.5147232717849082</td>
    </tr>
  </tbody>
</table>

<h2 id="final-model">Final Model</h2>

<h3 id="description"><strong>Description</strong></h3>

<p><code class="language-plaintext highlighter-rouge">recipe_num_ratings</code>: For the number of ratings for each recipe, we want to study whether a recipe with a higher number of ratings possibly tends to receive higher ratings from people.</p>

<p><code class="language-plaintext highlighter-rouge">user_average_rating</code>: For the user’s average rating, we want to check if a person tends to give a higher rating just because he habitually gives high scores.</p>

<h3 id="performance-1"><strong>Performance</strong></h3>

<p>In crafting our predictive model, we incorporated two additional features: <code class="language-plaintext highlighter-rouge">user_average_rating</code> and <code class="language-plaintext highlighter-rouge">recipe_num_ratings</code>. The inclusion of <code class="language-plaintext highlighter-rouge">user_average_rating</code> is predicated on the assumption that individual rating behaviors are consistent across multiple reviews. Thus, a user who tends to give higher ratings might similarly rate new recipes they encounter with high ratings. This feature captures the individual bias of users, which is fundamental to understanding the variability in recipe ratings.</p>

<p>The second feature, <code class="language-plaintext highlighter-rouge">recipe_num_ratings</code>, is grounded in the theory of wisdom of the crowds, which states that the information collection in groups results in decisions that are often better than the decisions made by any single member of the group. This implies that recipes with a large number of ratings are likely to have a more reliable rating, indicating that a large number of ratings on one recipe tends to have higher ratings than those with a smaller number of ratings.</p>

<p>For our predictive modeling, we also tried the Decision Tree Regressor model, a non-linear model that is well-suited for capturing complex patterns in data which linear models might miss. Decision trees are particularly adept at modeling interactions between different features, which we hypothesized would be present in our dataset.</p>

<p>Then we fine-tuned our model using GridSearch to optimize the hyperparameters <code class="language-plaintext highlighter-rouge">max_depth</code> and <code class="language-plaintext highlighter-rouge">min_samples_split</code>. This approach systematically works through multiple combinations of parameter tunes, cross-validating as it goes to determine which tune gives the best performance. We evaluated 70 different combinations, seeking to strike a balance between a model that is complex enough to learn the data well, but not so complex that it overfits. Finally, we got the best performance with the <code class="language-plaintext highlighter-rouge">max_depth</code> of 10 and the <code class="language-plaintext highlighter-rouge">min_samples_split</code> of 100.</p>

<p>Next, we calculated and compared the <strong>RMSE</strong> of the testing sets for each model as the table shown below. We found the DecisionTreeRegressor with the best hyperparameter has the lowest RMSE, so we chose this to be our final model.</p>

<table>
  <thead>
    <tr>
      <th>Metric</th>
      <th>Test RMSE</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>‘RMSE for regression with two features’</td>
      <td>0.5147232717849082</td>
    </tr>
    <tr>
      <td>‘RMSE for regression with three features’</td>
      <td>0.3758448690196555</td>
    </tr>
    <tr>
      <td>‘RMSE for regression with four features’</td>
      <td>0.3758027040111092</td>
    </tr>
    <tr>
      <td>‘RMSE for the DecisionTreeRegressor’</td>
      <td>0.43897293968980095</td>
    </tr>
    <tr>
      <td>‘RMSE for the DecisionTreeRegressor with the best hyperparameter’</td>
      <td>0.3215798723839845</td>
    </tr>
  </tbody>
</table>

<iframe src="assets/fig.html" width="800" height="600" frameborder="0"></iframe>
<p>As we can see from the plot below, our final model’s performance was an improvement over our baseline model, which we can infer our baseline model was a simpler and a basic linear regression model without these additional features or a non-tuned decision tree. The final Decision Tree Regressor with optimal hyperparameters yielded the lowest RMSE on the testing set, indicating that it was the most accurate at predicting recipe ratings. The reduction in RMSE from the baseline to the final model signifies an enhancement in predictive accuracy, likely due to the model’s increased complexity and ability to capture more subtle patterns within the data.</p>

<h2 id="fairness-analysis">Fairness Analysis</h2>

<h3 id="permutation-test">Permutation Test</h3>
<ul>
  <li>
    <p><strong>Null Hypothesis:</strong> Our model is fair. Its</p>
  </li>
  <li><strong>Alternative Hypothesis:</strong></li>
  <li><strong>Test Statistic:</strong></li>
  <li>
    <h2 id="significance-level-to-ensure-the-accuracy-of-our-conclusion-we-decided-to-use-a-significance-level-of-5-as-our-significance-level"><strong>Significance Level:</strong> To ensure the accuracy of our conclusion, we decided to use a significance level of 5% as our significance level.</h2>
  </li>
</ul>


      <footer class="site-footer">
        
          <span class="site-footer-owner"><a href="https://github.com/VickyShu/Recipes-Rating-Prediction">Recipes-Rating-Prediction</a> is maintained by <a href="https://github.com/VickyShu">VickyShu</a>.</span>
        
        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a>.</span>
      </footer>
    </main>
  </body>
</html>
